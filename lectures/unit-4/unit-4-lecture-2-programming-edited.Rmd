---
title: 'Unit 4 Lecture 2: Pruning and cross-validating decision trees (edited, and with solutions)'
date: "April 6, 2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

Today, we will learn how to select the complexity of decision trees based on cost complexity pruning and cross-validation, as implemented in the `rpart` package.

First, let's load some libraries:
```{r, message = FALSE}
library(rpart)             # install.packages("rpart")
library(rpart.plot)        # install.packages("rpart.plot")
library(tidyverse)
```

# Regression trees

Like last time, we will be using the `Hitters` data from the `ISLR` package, splitting into training and testing:
```{r, message = FALSE}
Hitters = ISLR2::Hitters %>% 
  as_tibble() %>% 
  filter(!is.na(Salary)) %>%   # remove NA values (in general not necessary)
  mutate(Salary = log(Salary)) # log-transform the salary
Hitters

set.seed(1) # set seed for reproducibility
train_samples = sample(1:nrow(Hitters), round(0.8*nrow(Hitters)))
Hitters_train = Hitters %>% filter(row_number() %in% train_samples)
Hitters_test = Hitters %>% filter(!(row_number() %in% train_samples))
```

As before, we fit a regression tree by calling `rpart`:
```{r}
tree_fit = rpart(Salary ~ ., data = Hitters_train)
```

## Tree pruning and cross validation

It turns out that in addition to growing the tree, behind the scenes `rpart` has already:

- used cost complexity pruning to get the nested sequence of trees
- applied 10-fold cross-validation to compute the CV estimates and standard errors for each value of $\alpha$

All we need to do is call the `printcp` function to get a summary of all this information:
```{r}
printcp(tree_fit)
```
Let's focus on the table at the bottom of this output. Each row corresponds to a tree in the sequence obtained by pruning. Let's discuss each column in turn:

- The `CP` column is the "complexity parameter". It is related to, but not exactly the same as, the $\alpha$ parameter from the slides. Be careful! The terminology "complexity parameter" is a bit misleading because higher complexity parameters correspond to less complex models (just like lambda in penalized regression). 
- `nsplit` is the number of splits in the tree. Note that `1+nsplit` is the number of terminal nodes in the tree.
- `rel error` is the RSS training error of the tree, normalized by the total variance of the response; equivalently, this is $1-R^2$. The training error decreases as the complexity increases.
- `xerror` is the cross-validation error estimate.
- `xstd` is the cross-validation standard error.

The exact values of the complexity parameter are not so important; we might as well parameterize the trees based on the number of terminal nodes. Armed with all this information, we can produce a CV plot. The built-in function to produce the CV plot is not as nice as the one built into `cv.glmnet`, so we'll make our own using `ggplot`:
```{r}
cp_table = printcp(tree_fit) %>% as_tibble()
cp_table %>% 
  ggplot(aes(x = nsplit+1, y = xerror, 
             ymin = xerror - xstd, ymax = xerror + xstd)) + 
  geom_point() + geom_line() +
  geom_errorbar(width = 0.2) +
  xlab("Number of terminal nodes") + ylab("CV error") + 
  geom_hline(aes(yintercept = min(xerror)), linetype = "dashed") + 
  theme_bw()
```

Audience participation: How many terminal nodes would we choose based on the one-standard-error rule? **We would choose three terminal nodes.**

Unfortunately, we don't have a convenient lambda.1se field of the output to directly extract the optimal complexity parameter based on the one standard error rule. Nevertheless, we can find it pretty simply using `dplyr`:
```{r}
optimal_tree_info = cp_table %>% 
  filter(xerror - xstd < min(xerror)) %>% 
  arrange(nsplit) %>% 
  head(1)
optimal_tree_info
```
Audience participation: What is the above code is doing? Why is `nsplit` two rather than three as suggested by the plot above?

**The code first identifies the rows of the tibble for which the lower endpoint of the interval (`xerror - xstd`) is below the minimum CV value (`xerror`), then sorts according to `nsplit`, so that less complex trees appear first, and then takes the first value to extract the tree among those remaining with the fewest number of terminal nodes. `nsplit` is two rather than three because two splits corresponds to three terminal nodes.**

## Extracting the pruned tree and making predictions

To actually get the optimal pruned tree, we need to use the function `prune`, specifying the complexity parameter 

```{r}
optimal_tree = prune(tree_fit, cp = optimal_tree_info$CP)
```

As before, we can plot this tree using `rpart.plot`:
```{r}
rpart.plot(optimal_tree)
```

That is a small tree! In the bias variance trade-off, sometimes less (complexity) is more (predictive performance). 

Now we can make predictions on the test data and evaluate MSE using this tree:
```{r}
pred = predict(optimal_tree, newdata = Hitters_test)
pred

mean((pred-Hitters_test$Salary)^2)
```

# Exercise: Classification trees

Let's continue with the heart disease data from last time: 
```{r, message = FALSE, warning = FALSE}
# download the data
url = "https://raw.githubusercontent.com/JWarmenhoven/ISLR-python/master/Notebooks/Data/Heart.csv"
Heart = read_csv(url, col_types = "-iffiiiiiddiiff")

# split into train/test
set.seed(1) # set seed for reproducibility
train_samples = sample(1:nrow(Heart), round(0.8*nrow(Heart)))
Heart_train = Heart %>% filter(row_number() %in% train_samples)
Heart_test = Heart %>% filter(!(row_number() %in% train_samples))

# fit a classification tree
tree_fit = rpart(AHD ~ ., 
                 method = "class",              # classification 
                 parms = list(split = "gini"),  # Gini index for splitting
                 data = Heart_train)
```

## Tree pruning and cross-validation

1. Produce the table of the trees in the sequence obtained from cost complexity pruning.

```{r}
printcp(tree_fit)
```
Question: What exactly is the interpretation of the `CP` column in this case? Do the values make sense? 

**The `CP` column is the complexity parameter. It is related to, but not exactly the same as, the alpha from the slides. NOTE: Interpreting this parameter exactly is beyond the scope of the course.**


2. Produce the CV plot. How many terminal nodes would we choose based on the one-standard-error rule? Do we notice anything strange about the CV plot?

```{r}
cp_table = printcp(tree_fit) %>% as_tibble()
cp_table %>% 
  ggplot(aes(x = nsplit+1, y = xerror, 
             ymin = xerror - xstd, ymax = xerror + xstd)) + 
  geom_point() + geom_line() +
  geom_errorbar(width = 0.2) +
  xlab("Number of terminal nodes") + ylab("CV error") + 
  geom_hline(aes(yintercept = min(xerror)), linetype = "dashed") + 
  theme_bw()
```

**We would choose two terminal nodes. One strange thing in the CV plot is that there are "gaps" at 3 and 5 terminal nodes. This occurs because internal nodes, rather than leaf nodes, were the weakest link at those two places in the pruning algorithm, decreasing the number of terminal nodes by two rather than by one. NOTE: Understanding this phenomenon is beyond the scope of the course.**


3. Extract and visualize the tree chosen by cross-validation. In words, how would you summarize the resulting decision rule? 

```{r}
optimal_tree_info = cp_table %>% 
  filter(xerror - xstd < min(xerror)) %>% 
  arrange(nsplit) %>% 
  head(1)

optimal_tree_info

optimal_tree = prune(tree_fit, cp = optimal_tree_info$CP)

rpart.plot(optimal_tree)
```

**This decision tree classifies `No` if a patient's chest pain falls into one of the following categories: nonanginal, nontypical, typical. It also classifies `No` if chest pain is in the remaining categories, `Thal = normal` and `Ca < 1`. Otherwise it classifies `Yes`.**

4. What is the test misclassification error of this decision rule? 

```{r}
pred = predict(optimal_tree, newdata = Heart_test, type = "class")
pred

mean(pred != Heart_test$AHD)
```